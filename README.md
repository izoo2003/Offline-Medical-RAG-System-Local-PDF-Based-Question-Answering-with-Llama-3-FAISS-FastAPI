
# ğŸ“˜ Offline Medical RAG System

**Local PDF-Based Question Answering using Llama 3, FAISS, FastAPI, and Streamlit**

A fully offline Retrieval-Augmented Generation (RAG) application that allows users to upload medical PDFs and ask natural-language questions about them. Built using local models via **Ollama**, semantic search through **FAISS**, embeddings using **Sentence Transformers**, a **FastAPI** backend, and a clean **Streamlit** web interface.

No cloud APIs, no paid services, and no internet connection required.

---

## âœ¨ Features

* ğŸ“„ **Upload Medical PDFs** and extract text automatically
* ğŸ” **Semantic Search with FAISS** for high-accuracy retrieval
* ğŸ§  **Local Llama 3.2 via Ollama** for fully offline LLM responses
* ğŸ§© **RAG Pipeline** (retrieve â†’ augment â†’ generate)
* âš¡ **FastAPI Backend** exposing clean REST endpoints
* ğŸ¨ **Streamlit Frontend** for real-time interactive Q&A
* ğŸ”’ **Zero Cloud Dependencies** (completely private & local)
* ğŸ§ª **Extendable & Modular Architecture**

---

## ğŸ—ï¸ Project Architecture

```
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚     Streamlit UI     â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚    FastAPI API    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        Upload PDF       â”‚        Ask Question
                          â†“
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚        Ingestion Pipeline     â”‚
           â”‚  (PDF â†’ Text â†’ Chunks â†’ Embed)â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚       FAISS DB       â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
               Retrieve Top Chunks
                       â†“
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚      Llama 3 (Ollama)   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Technologies Used

### ğŸ”¹ **Core AI/RAG**

* **Sentence Transformers** â€“ Text embeddings
* **FAISS** â€“ Vector similarity search
* **Llama 3.2 (Ollama)** â€“ Local LLM inference
* **Custom Chunking & Retrieval Logic**

### ğŸ”¹ **Backend**

* **FastAPI** â€“ REST API
* **Uvicorn** â€“ ASGI server

### ğŸ”¹ **Frontend**

* **Streamlit** â€“ Real-time interactive UI

### ğŸ”¹ **Utilities**

* **pypdf** â€“ PDF text extraction
* **NumPy** â€“ Vector normalization
* **Requests** â€“ API communication

---

## ğŸ“¦ Installation & Setup

### 1ï¸âƒ£ Clone the Repository

```
git clone https://github.com/your-username/offline-medical-rag.git
cd offline-medical-rag
```

### 2ï¸âƒ£ Create Virtual Environment

```
python -m venv venv
venv\Scripts\activate   # Windows
```

### 3ï¸âƒ£ Install Dependencies

```
pip install -r requirements.txt
```

### 4ï¸âƒ£ Install & Run Ollama Model

Install Ollama: [https://ollama.com](https://ollama.com)
Then pull the model:

```
ollama pull llama3.2
```

### 5ï¸âƒ£ Start FastAPI Backend

```
uvicorn app.main:app --reload
```

### 6ï¸âƒ£ Start Streamlit Frontend

Open a second terminal, re-activate venv:

```
venv\Scripts\activate
streamlit run ui/app.py
```

UI will open on **[http://localhost:8501](http://localhost:8501)**

---

## ğŸš€ Usage

### **Upload Documents**

* Upload one or multiple PDFs
* System ingests â†’ chunks â†’ embeds â†’ stores in FAISS

### **Ask Questions**

Type questions like:

* â€œSummarize this document.â€
* â€œWhat symptoms are described?â€
* â€œWhat medications are mentioned?â€

System retrieves best chunks and Llama generates the answer.

---

## ğŸ§  How RAG Works in This Project

1. PDF uploaded
2. Text extracted using pypdf
3. Text split into semantic chunks
4. Chunks embedded using `all-MiniLM-L6-v2`
5. Stored in FAISS for similarity search
6. User asks a question â†’ embedded
7. FAISS retrieves most relevant chunks
8. Llama 3.2 generates answer using retrieved context

---

## ğŸ—‚ï¸ Project Structure

```
offline-medical-rag/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ vectorstore.py
â”‚   â”œâ”€â”€ document_loader.py
â”‚   â””â”€â”€ rag_engine.py
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“ˆ Future Enhancements

* Chat history
* â€œShow retrieved sourcesâ€
* Advanced chunking (semantic splitting)
* Hybrid search (BM25 + vectors)
* PDF page previews
* Persistent FAISS storage

---

## ğŸ¤ Contributing

Pull requests and ideas are welcome!

---
